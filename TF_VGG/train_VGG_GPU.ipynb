{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from VGGModel.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from VGGModel import vgg\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_DEVICE_ORDER'] = \"PCI_BUS_ID\"  # os.environ[“CUDA_DEVICE_ORDER”] = “PCI_BUS_ID” # 按照PCI_BUS_ID顺序从0开始排列GPU设备\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3'        # 设置当前使用的GPU设备仅为0号设备  设备名称为'/gpu:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "logical_gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu,True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "        exit(-1)\n",
    "logical_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = os.path.abspath(os.path.join(os.getcwd(),\"../../datasets\"))\n",
    "image_path = data_root+\"/flower_data/\"\n",
    "train_dir = image_path+'train'\n",
    "validation_dir = image_path+'val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_height = 224\n",
    "im_width = 224\n",
    "batch_size = 32\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('save_weights'):\n",
    "    os.makedirs(\"save_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'dandelion', 1: 'daisy', 2: 'roses', 3: 'tulips', 4: 'sunflowers'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class dict\n",
    "data_class = [cla for cla in os.listdir(train_dir) if '.txt' not in cla]\n",
    "class_num = len(data_class)\n",
    "class_dict = dict((value,index) for index,value in enumerate(data_class))\n",
    "inverse_dict = dict((value,key) for key,value in class_dict.items())\n",
    "inverse_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train images list\n",
    "train_image_list = glob.glob(train_dir+\"/*/*.jpg\")\n",
    "random.shuffle(train_image_list)\n",
    "train_num = len(train_image_list)\n",
    "train_label_list = [class_dict[path.split(os.path.sep)[-2]] for path in train_image_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load validation images list\n",
    "val_image_list = glob.glob(validation_dir+\"/*/*.jpg\")\n",
    "random.shuffle(val_image_list)\n",
    "val_num = len(val_image_list)\n",
    "val_label_list = [class_dict[path.split(os.path.sep)[-2]] for path in val_image_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_path(img_path,label):\n",
    "    label = tf.one_hot(label,depth=class_num)\n",
    "    image = tf.io.read_file(img_path)\n",
    "    image = tf.image.decode_jpeg(image)\n",
    "    image = tf.image.convert_image_dtype(image,tf.float32)\n",
    "    image = tf.image.resize(image,[im_height,im_width])\n",
    "    return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "# load train dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_image_list, train_label_list))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=train_num)\\\n",
    "                                 .map(process_path, num_parallel_calls=AUTOTUNE)\\\n",
    "                                 .repeat().batch(batch_size).prefetch(AUTOTUNE)\n",
    "\n",
    "# load train dataset\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_image_list, val_label_list))\n",
    "val_dataset = val_dataset.map(process_path, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
    "                             .repeat().batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "feature (Sequential)         (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 2048)              51382272  \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 2048)              4196352   \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 5)                 10245     \n",
      "_________________________________________________________________\n",
      "softmax_6 (Softmax)          (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 70,303,557\n",
      "Trainable params: 70,303,557\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "32.08864722191356\n",
      "Epoch 0,loss: 1.6034200191497803,Accuracy: 23.816747665405273,Test Loss: 1.6019703149795532\n",
      "31.968229926889762\n",
      "Epoch 1,loss: 1.6009405851364136,Accuracy: 25.394418716430664,Test Loss: 1.5742284059524536\n",
      "31.87971956189722\n",
      "Epoch 2,loss: 1.5176410675048828,Accuracy: 31.219661712646484,Test Loss: 1.4495457410812378\n",
      "31.66119160503149\n",
      "Epoch 3,loss: 1.3915033340454102,Accuracy: 39.92718505859375,Test Loss: 1.3268158435821533\n",
      "31.93910855310969\n",
      "Epoch 4,loss: 1.2741302251815796,Accuracy: 46.78398132324219,Test Loss: 1.2660692930221558\n",
      "31.96698184707202\n",
      "Epoch 5,loss: 1.1690735816955566,Accuracy: 51.699031829833984,Test Loss: 1.0503324270248413\n",
      "31.708067243918777\n",
      "Epoch 6,loss: 1.0366426706314087,Accuracy: 59.43567657470703,Test Loss: 0.9356380105018616\n",
      "31.808782673906535\n",
      "Epoch 7,loss: 0.9203187227249146,Accuracy: 63.319175720214844,Test Loss: 0.9189066290855408\n",
      "31.720671470975503\n",
      "Epoch 8,loss: 0.8456989526748657,Accuracy: 66.8992691040039,Test Loss: 0.9020753502845764\n",
      "31.78335312078707\n",
      "Epoch 9,loss: 0.7894724607467651,Accuracy: 70.0242691040039,Test Loss: 0.8593748211860657\n",
      "31.640798300970346\n",
      "Epoch 10,loss: 0.7252002954483032,Accuracy: 71.90534210205078,Test Loss: 0.8469043970108032\n"
     ]
    }
   ],
   "source": [
    "# 单GPU\n",
    "model=vgg('vgg16',224,224,5)\n",
    "model.summary()\n",
    "    \n",
    "# use keras low level api for training\n",
    "loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    \n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "    \n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "    \n",
    "@tf.function\n",
    "def train_step(images,labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images,training=True)\n",
    "        loss = loss_object(labels,predictions)\n",
    "    gradients = tape.gradient(loss,model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients,model.trainable_variables))\n",
    "        \n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels,predictions)\n",
    "        \n",
    "@tf.function\n",
    "def test_step(images,labels):\n",
    "    predictions = model(images,training=False)\n",
    "    t_loss = loss_object(labels,predictions)\n",
    "        \n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels,predictions)\n",
    "        \n",
    "best_test_loss = float('inf')\n",
    "train_step_num = train_num//batch_size\n",
    "val_step_num = val_num//batch_size\n",
    "for epoch in range(epochs+1):\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "        \n",
    "    t1 = time.perf_counter()\n",
    "    for index,(images,labels) in enumerate(train_dataset):\n",
    "        train_step(images,labels)\n",
    "        if index+1 == train_step_num:\n",
    "            break\n",
    "    print(time.perf_counter()-t1)\n",
    "        \n",
    "    for index,(images,labels) in enumerate(val_dataset):\n",
    "        test_step(images,labels)\n",
    "        if index+1 == val_step_num:\n",
    "            break\n",
    "    template = 'Epoch {},loss: {},Accuracy: {},Test Loss: {} ,Test Accuracy: {}'\n",
    "    print(template.format(epoch,\n",
    "                        train_loss.result(),\n",
    "                        train_accuracy.result()*100,\n",
    "                        test_loss.result(),\n",
    "                        test_accuracy.result()*100))\n",
    "    if test_loss.result() < best_test_loss:\n",
    "        model.save_weights('./save_weights/myVGG_{}.ckpt'.format(epoch),save_format='tf')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size_per_replica = 32\n",
    "# # Global batch size\n",
    "# GLOBAL_BATCH_SIZE = batch_size_per_replica * strategy.num_replicas_in_sync\n",
    "# # Buffer size for data loader\n",
    "# BUFFER_SIZE = batch_size_per_replica * strategy.num_replicas_in_sync * 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分布式多GPU\n",
    "# load train dataset\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with strategy.scope():\n",
    "    batch_size_per_replica = 32\n",
    "    batch_size= batch_size_per_replica*len(logical_gpus)\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_image_list,train_label_list))\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=train_num)\\\n",
    "                                .map(process_path,num_parallel_calls=AUTOTUNE)\\\n",
    "                                .repeat().batch(batch_size).prefetch(AUTOTUNE)\n",
    "    \n",
    "    \n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((val_image_list,val_label_list))\n",
    "    val_dataset = val_dataset.map(process_path,num_parallel_calls=AUTOTUNE).repeat().batch(batch_size)\n",
    "    \n",
    "    train_dataset_distribute = strategy.experimental_distribute_dataset(train_dataset)\n",
    "    val_dataset_distribute = strategy.experimental_distribute_dataset(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "feature (Sequential)         (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2048)              51382272  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2048)              4196352   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 10245     \n",
      "_________________________________________________________________\n",
      "softmax (Softmax)            (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 70,303,557\n",
      "Trainable params: 70,303,557\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.\n",
      "INFO:tensorflow:batch_all_reduce: 32 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.\n",
      "INFO:tensorflow:batch_all_reduce: 32 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.\n",
      "INFO:tensorflow:batch_all_reduce: 32 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.\n",
      "INFO:tensorflow:batch_all_reduce: 32 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.\n",
      "INFO:tensorflow:batch_all_reduce: 32 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:batch_all_reduce: 32 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:batch_all_reduce: 32 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:batch_all_reduce: 32 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:batch_all_reduce: 32 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:batch_all_reduce: 32 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "156.05143028707244\n",
      "Epoch 0,loss: 1.601108431816101,Accuracy: 23.640625,Test Loss: 1.6019304990768433,Test Accuracy: 30.46875\n",
      "125.39058624906465\n",
      "Epoch 1,loss: 1.5992587804794312,Accuracy: 26.40625,Test Loss: 1.585545301437378,Test Accuracy: 23.4375\n",
      "125.05242318799719\n",
      "Epoch 2,loss: 1.4927557706832886,Accuracy: 29.15625,Test Loss: 1.4127399921417236,Test Accuracy: 35.15625\n",
      "124.81734251393937\n",
      "Epoch 3,loss: 1.3289268016815186,Accuracy: 40.359375,Test Loss: 1.251673936843872,Test Accuracy: 44.140625\n",
      "125.0287865260616\n",
      "Epoch 4,loss: 1.2633755207061768,Accuracy: 44.703125,Test Loss: 1.164099931716919,Test Accuracy: 48.046875\n",
      "125.15035225101747\n",
      "Epoch 5,loss: 1.1779652833938599,Accuracy: 49.078125,Test Loss: 1.0834414958953857,Test Accuracy: 55.078125\n",
      "124.95754307694733\n",
      "Epoch 6,loss: 1.0822291374206543,Accuracy: 55.757816314697266,Test Loss: 1.0728774070739746,Test Accuracy: 60.15625\n",
      "124.87752971798182\n",
      "Epoch 7,loss: 1.0176187753677368,Accuracy: 59.65625,Test Loss: 0.9653347134590149,Test Accuracy: 61.328125\n",
      "124.89604764897376\n",
      "Epoch 8,loss: 0.9587620496749878,Accuracy: 61.578125,Test Loss: 0.9131110906600952,Test Accuracy: 66.015625\n",
      "124.52394848293625\n",
      "Epoch 9,loss: 0.8852675557136536,Accuracy: 64.640625,Test Loss: 0.923903226852417,Test Accuracy: 64.0625\n",
      "124.89547000383027\n",
      "Epoch 10,loss: 0.8011027574539185,Accuracy: 68.90625,Test Loss: 0.8912298083305359,Test Accuracy: 68.75\n"
     ]
    }
   ],
   "source": [
    "# 分布式多GPU\n",
    "with strategy.scope():\n",
    "    model=vgg('vgg16',224,224,5)\n",
    "    model.summary()\n",
    "    \n",
    "    loss_object = tf.keras.losses.CategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    def comput_loss(labels,predictions):\n",
    "        per_replica_loss = loss_object(labels,predictions)\n",
    "        return tf.nn.compute_average_loss(per_replica_loss,global_batch_size=batch_size)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "    train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "    test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "    test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "\n",
    "    def train_step(images,labels):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(images,training=True)\n",
    "            loss = comput_loss(labels,predictions)\n",
    "        gradients = tape.gradient(loss,model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients,model.trainable_variables))\n",
    "\n",
    "        train_loss(loss)\n",
    "        train_accuracy(labels,predictions)\n",
    "        return loss\n",
    "        \n",
    "    def distributed_train_step(images,labels):\n",
    "        per_replica_average_loss = strategy.experimental_run_v2(train_step,args=(images,labels,))\n",
    "        return strategy.reduce(tf.distribute.ReduceOp.SUM,per_replica_average_loss,axis=None)\n",
    "\n",
    "\n",
    "    def test_step(images,labels):\n",
    "        predictions = model(images,training=False)\n",
    "        t_loss = loss_object(labels,predictions)\n",
    "\n",
    "        test_loss(t_loss)\n",
    "        test_accuracy(labels,predictions)\n",
    "        \n",
    "    def distributed_val_step(image,labels):\n",
    "        strategy.experimental_run_v2(test_step,args=(image,labels,))\n",
    "\n",
    "    best_test_loss = float('inf')\n",
    "    train_step_num = train_num//batch_size\n",
    "    val_step_num = val_num//batch_size\n",
    "    for epoch in range(epochs+1):\n",
    "        train_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "        test_loss.reset_states()\n",
    "        test_accuracy.reset_states()\n",
    "        \n",
    "        t1 = time.perf_counter()\n",
    "        for index,(images,labels) in enumerate(train_dataset):\n",
    "            distributed_train_step(images,labels)\n",
    "            if index+1 == train_step_num:\n",
    "                break\n",
    "        print(time.perf_counter()-t1)\n",
    "        \n",
    "\n",
    "        for index,(images,labels) in enumerate(val_dataset):\n",
    "            distributed_val_step(images,labels)\n",
    "            if index+1 == val_step_num:\n",
    "                break\n",
    "                \n",
    "        template = 'Epoch {},loss: {},Accuracy: {},Test Loss: {},Test Accuracy: {}'\n",
    "        print(template.format(epoch,\n",
    "                            train_loss.result(),\n",
    "                            train_accuracy.result()*100,\n",
    "                            test_loss.result(),\n",
    "                            test_accuracy.result()*100))\n",
    "        if test_loss.result() < best_test_loss:\n",
    "            model.save_weights('./save_weights/myVGG_GPUS_{}.ckpt'.format(epoch),save_format='tf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
