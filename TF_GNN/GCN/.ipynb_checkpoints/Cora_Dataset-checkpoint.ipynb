{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORA数据集由机器学习论文组成。这些文件可分为以下七个类别之一：\n",
    "    基于案例的\n",
    "    遗传算法\n",
    "    神经网络\n",
    "    概率方法\n",
    "    强化学习\n",
    "    规则_学习\n",
    "    理论\n",
    "    \n",
    "    这些论文的选择方式是，在最终语料库中，每篇论文至少引用一篇其他论文或被至少一篇其他论文引用。\n",
    "    整个语料库共有2708篇论文。\n",
    "    在对停顿词进行词干处理和删除之后，我们剩下1433个独特单词的词汇表。文档频率低于10的所有单词都被删除。\n",
    "    \n",
    "    该目录包含两个文件：\n",
    "        .content文件包含以下格式的论文说明：<Paper_id><word_tributes>+<class_label>\n",
    "        \n",
    "    每行的第一个条目包含论文的唯一字符串ID，后跟指示词汇表中的每个单词在论文中是否存在(由1表示)或不存在(由0表示)的二进制值。\n",
    "    最后，该行的最后一个条目包含纸张的类别标签。\n",
    "    每行的数据格式如下: <paper_id> <word_attributes>+ <class_label>。paper id是论文的唯一标识；word_attributes是是一个维度为1433的词向量，\n",
    "    词向量的每个元素对应一个词，0表示该元素对应的词不在Paper中，1表示该元素对应的词在Paper中。class_label是论文的类别，\n",
    "    每篇Paper被映射到如下7个分类之一: Case_Based、Genetic_Algorithms、Neural_Networks、Probabilistic_Methods、Reinforcement_Learning、Rule_Learning、Theory。\n",
    "    \n",
    "        .cites文件包含语料库的引文图表。每行以以下格式描述一个链接：<被引用论文ID><引用论文ID>\n",
    "        \n",
    "    每行包含两个纸质ID。第一个条目是被引用论文的ID，第二个ID代表包含引文的论文。链接的方向是从右到左。如果一行由“Pap1 Pap2”表示，则链接为“Pap2->Pap1”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoraData():\n",
    "    def __init__(self, data_root='/home/zuoyuhui/datasets/cora/'):\n",
    "        self._data_root = data_root\n",
    "        self._data = self.process_data()\n",
    "        \n",
    "    def load_data(self,dataset='cora'):\n",
    "        print('Loading {} dataset...'.format(dataset))\n",
    "        idx_features_labels = np.genfromtxt(\"{}{}.cites\".format(self._data_root,dataset),dtype=np.dtype(str))\n",
    "        edges = np.genfromtxt(\"{}{}.cites\".format(self._data_root,dataset),dtype=np.int32)\n",
    "        return idx_features_labels, edges\n",
    "    \n",
    "    def process_data(self):\n",
    "        print(\"Process data...\")\n",
    "        \n",
    "        idx_features_labels, edges = self.load_data()\n",
    "        \n",
    "        features = idx_features_labels[:,1:-1].astype(np.float32)\n",
    "        features = self.normalize_feature(features)\n",
    "        \n",
    "        y = idx_features_labels[:,-1]\n",
    "        labels = self.encode_onehot(y)\n",
    "        \n",
    "        idx = np.array(idx_features_labels[:,0],dtype=np.int32)\n",
    "        idx_map = {j:i for i,j in enumerate(idx)}\n",
    "        edge_indexs = np.array(list(map(idx_map.get,edges.flatten())),dtype=np.int32)\n",
    "        edge_indexs = edge_indexs.reshape(edges.shape)\n",
    "        \n",
    "        edge_index_len =len(edge_indexs)\n",
    "        for i in range(edge_index_len):\n",
    "            edge_indexs = np.concatenate((edge_indexs,[[edge_indexs[i][1],edge_indexs[i][0]]]))\n",
    "        \n",
    "        adjacency = sp.coo_matrix((np.ones(len(edge_indexs)),\n",
    "                                  (edge_indexs[:,0],edge_indexs[:,1])),\n",
    "                                 shape=(features.shape[0],features.shape[0]),dtype='float32')\n",
    "        adjacency = self.normalize_adj(adjacency)\n",
    "        \n",
    "        train_index = np.arange(150)\n",
    "        val_index = np.arange(150,500)\n",
    "        test_index = np.arange(500,2708)\n",
    "        \n",
    "        train_mask = np.zeros(edge_indexs.shape[0],dtype=np.bool)\n",
    "        val_mask = np.zeros(edge_indexs.shape[0],dtype=np.bool)\n",
    "        test_mask = np.zeros(edge_indexs.shape[0],dtype=np.bool)\n",
    "        train_mask[train_index]=True\n",
    "        val_mask[val_index]=True\n",
    "        test_mask[test_index]=True\n",
    "        \n",
    "        print('Dataset has {} nodes, {} edges, {} features.'.format(features.shape[0], adjacency.shape[0], features.shape[1]))\n",
    "\n",
    "        return features, labels, adjacency, train_mask, val_mask, test_mask\n",
    "        \n",
    "        \n",
    "    def encode_onehot(self,label):\n",
    "        classes = set(labels)\n",
    "        class_dict = {c: np.identity(len(classes))[i,:] for i,c in enumerate(classes)}\n",
    "        labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
    "        return labels_onehot\n",
    "    \n",
    "    def normalize_feature(self,features):\n",
    "        noraml_features = features/features.sum(1).reshape(-1,1)\n",
    "        return noraml_features\n",
    "    \n",
    "    def normalize_adj(self,adjacency):\n",
    "        \"\"\"计算 L=D^-0.5 * (A+I) * D^-0.5\"\"\"\n",
    "        adjacency += sp.eye(adjacency.shape[0]) #增加自连接\n",
    "        degree = np.array(adjacency.sum(1))\n",
    "        d_hat = sp.diags(np.power(degree,-0.5).flatten())\n",
    "        return d_hat.dot(adjacency).dot(d_hat).tocsr().todense()\n",
    "    \n",
    "    def data(self):\n",
    "        \"\"\"返回Data数据对象，包括features, labes, adjacency, train_mask, val_mask, test_mask\"\"\"\n",
    "        return self._data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
