{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch tensorss\n",
    "钱箱神经网络，计算损失，反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "N=64 # 64个输入\n",
    "D_in=1000 #输入1000维\n",
    "H=100 # 中间层 100维\n",
    "D_out=10 #输出10维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机创建一些训练数据\n",
    "x = torch.randn(N,D_in)\n",
    "y = torch.randn(N,D_out)\n",
    "\n",
    "w1 = torch.randn(D_in,H,requires_grad=True) # tensor专门列开一个位置叫grad，这样算backword 会把grad填上\n",
    "w2 = torch.randn(H,D_out,requires_grad=True)\n",
    "\n",
    "learning_rate=1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 24442182.0\n",
      "1 18017100.0\n",
      "2 14943171.0\n",
      "3 13031245.0\n",
      "4 11426987.0\n",
      "5 9721112.0\n",
      "6 7960073.0\n",
      "7 6232003.0\n",
      "8 4725415.5\n",
      "9 3491608.25\n",
      "10 2556210.5\n",
      "11 1868798.0\n",
      "12 1381075.25\n",
      "13 1036788.375\n",
      "14 795124.25\n",
      "15 623281.875\n",
      "16 499653.3125\n",
      "17 408858.34375\n",
      "18 340783.0\n",
      "19 288508.84375\n",
      "20 247478.609375\n",
      "21 214638.78125\n",
      "22 187817.6875\n",
      "23 165544.890625\n",
      "24 146794.859375\n",
      "25 130802.1640625\n",
      "26 117026.5\n",
      "27 105066.71875\n",
      "28 94625.90625\n",
      "29 85452.359375\n",
      "30 77356.609375\n",
      "31 70187.21875\n",
      "32 63811.05859375\n",
      "33 58121.1015625\n",
      "34 53033.43359375\n",
      "35 48471.16796875\n",
      "36 44378.49609375\n",
      "37 40693.78515625\n",
      "38 37368.171875\n",
      "39 34360.328125\n",
      "40 31636.92578125\n",
      "41 29167.78515625\n",
      "42 26924.416015625\n",
      "43 24881.5\n",
      "44 23018.91796875\n",
      "45 21318.880859375\n",
      "46 19764.314453125\n",
      "47 18340.71484375\n",
      "48 17036.35546875\n",
      "49 15839.859375\n",
      "50 14739.7802734375\n",
      "51 13728.0654296875\n",
      "52 12796.529296875\n",
      "53 11937.958984375\n",
      "54 11145.99609375\n",
      "55 10414.7958984375\n",
      "56 9738.666015625\n",
      "57 9112.7666015625\n",
      "58 8532.9736328125\n",
      "59 7995.7861328125\n",
      "60 7497.36669921875\n",
      "61 7034.40771484375\n",
      "62 6604.02099609375\n",
      "63 6203.76953125\n",
      "64 5831.232421875\n",
      "65 5484.0830078125\n",
      "66 5160.48974609375\n",
      "67 4858.50830078125\n",
      "68 4576.51611328125\n",
      "69 4313.14013671875\n",
      "70 4066.857177734375\n",
      "71 3836.486328125\n",
      "72 3620.8994140625\n",
      "73 3419.126708984375\n",
      "74 3230.1552734375\n",
      "75 3052.849609375\n",
      "76 2886.471435546875\n",
      "77 2730.19677734375\n",
      "78 2583.41455078125\n",
      "79 2445.384521484375\n",
      "80 2315.61474609375\n",
      "81 2193.477783203125\n",
      "82 2078.52099609375\n",
      "83 1970.246826171875\n",
      "84 1868.2005615234375\n",
      "85 1771.98974609375\n",
      "86 1681.2442626953125\n",
      "87 1595.6259765625\n",
      "88 1514.786865234375\n",
      "89 1438.4840087890625\n",
      "90 1366.4234619140625\n",
      "91 1298.32763671875\n",
      "92 1233.986572265625\n",
      "93 1173.1280517578125\n",
      "94 1115.5726318359375\n",
      "95 1061.158203125\n",
      "96 1009.6797485351562\n",
      "97 960.9306030273438\n",
      "98 914.72216796875\n",
      "99 870.9232788085938\n",
      "100 829.3823852539062\n",
      "101 789.9998779296875\n",
      "102 752.641845703125\n",
      "103 717.1795654296875\n",
      "104 683.5093383789062\n",
      "105 651.544189453125\n",
      "106 621.1763305664062\n",
      "107 592.3406372070312\n",
      "108 564.9344482421875\n",
      "109 538.8770141601562\n",
      "110 514.1122436523438\n",
      "111 490.5557861328125\n",
      "112 468.15130615234375\n",
      "113 446.84454345703125\n",
      "114 426.5541076660156\n",
      "115 407.2698059082031\n",
      "116 388.894287109375\n",
      "117 371.39483642578125\n",
      "118 354.75726318359375\n",
      "119 338.9089660644531\n",
      "120 323.80340576171875\n",
      "121 309.41766357421875\n",
      "122 295.70782470703125\n",
      "123 282.636962890625\n",
      "124 270.1742858886719\n",
      "125 258.2891845703125\n",
      "126 246.95660400390625\n",
      "127 236.14756774902344\n",
      "128 225.83154296875\n",
      "129 215.98410034179688\n",
      "130 206.5905303955078\n",
      "131 197.6241455078125\n",
      "132 189.06631469726562\n",
      "133 180.8992919921875\n",
      "134 173.10423278808594\n",
      "135 165.64926147460938\n",
      "136 158.5295867919922\n",
      "137 151.72901916503906\n",
      "138 145.23190307617188\n",
      "139 139.025146484375\n",
      "140 133.09780883789062\n",
      "141 127.42833709716797\n",
      "142 122.01101684570312\n",
      "143 116.83190155029297\n",
      "144 111.88021087646484\n",
      "145 107.14463806152344\n",
      "146 102.61959075927734\n",
      "147 98.29312896728516\n",
      "148 94.15291595458984\n",
      "149 90.19129943847656\n",
      "150 86.40509033203125\n",
      "151 82.7830810546875\n",
      "152 79.31916809082031\n",
      "153 76.00242614746094\n",
      "154 72.82903289794922\n",
      "155 69.79255676269531\n",
      "156 66.88499450683594\n",
      "157 64.1022720336914\n",
      "158 61.439605712890625\n",
      "159 58.8885498046875\n",
      "160 56.44800567626953\n",
      "161 54.111122131347656\n",
      "162 51.87382888793945\n",
      "163 49.73268508911133\n",
      "164 47.680633544921875\n",
      "165 45.714691162109375\n",
      "166 43.833282470703125\n",
      "167 42.030494689941406\n",
      "168 40.3065185546875\n",
      "169 38.652679443359375\n",
      "170 37.06861877441406\n",
      "171 35.550209045410156\n",
      "172 34.09550476074219\n",
      "173 32.70201873779297\n",
      "174 31.366119384765625\n",
      "175 30.08662986755371\n",
      "176 28.86060333251953\n",
      "177 27.684663772583008\n",
      "178 26.55830955505371\n",
      "179 25.478179931640625\n",
      "180 24.44394302368164\n",
      "181 23.4511775970459\n",
      "182 22.501068115234375\n",
      "183 21.58966636657715\n",
      "184 20.71623992919922\n",
      "185 19.87843894958496\n",
      "186 19.075265884399414\n",
      "187 18.304838180541992\n",
      "188 17.565866470336914\n",
      "189 16.85744857788086\n",
      "190 16.17805290222168\n",
      "191 15.526092529296875\n",
      "192 14.901670455932617\n",
      "193 14.302446365356445\n",
      "194 13.727702140808105\n",
      "195 13.175955772399902\n",
      "196 12.64756965637207\n",
      "197 12.140192985534668\n",
      "198 11.653922080993652\n",
      "199 11.187444686889648\n",
      "200 10.739767074584961\n",
      "201 10.31010627746582\n",
      "202 9.897880554199219\n",
      "203 9.502514839172363\n",
      "204 9.122713088989258\n",
      "205 8.759084701538086\n",
      "206 8.409494400024414\n",
      "207 8.074389457702637\n",
      "208 7.752594947814941\n",
      "209 7.443684101104736\n",
      "210 7.147360801696777\n",
      "211 6.863463401794434\n",
      "212 6.590871810913086\n",
      "213 6.328887462615967\n",
      "214 6.077395915985107\n",
      "215 5.836179733276367\n",
      "216 5.604877471923828\n",
      "217 5.3825788497924805\n",
      "218 5.1691131591796875\n",
      "219 4.964597225189209\n",
      "220 4.767840385437012\n",
      "221 4.5793585777282715\n",
      "222 4.397870063781738\n",
      "223 4.224155902862549\n",
      "224 4.057277202606201\n",
      "225 3.8970301151275635\n",
      "226 3.7432010173797607\n",
      "227 3.595458745956421\n",
      "228 3.453709602355957\n",
      "229 3.317322254180908\n",
      "230 3.1866304874420166\n",
      "231 3.0610249042510986\n",
      "232 2.940380573272705\n",
      "233 2.8246796131134033\n",
      "234 2.7132766246795654\n",
      "235 2.606657028198242\n",
      "236 2.5042834281921387\n",
      "237 2.4057912826538086\n",
      "238 2.311232566833496\n",
      "239 2.2205700874328613\n",
      "240 2.1333212852478027\n",
      "241 2.0495548248291016\n",
      "242 1.9690651893615723\n",
      "243 1.8918794393539429\n",
      "244 1.817711353302002\n",
      "245 1.7464419603347778\n",
      "246 1.677999496459961\n",
      "247 1.6122772693634033\n",
      "248 1.5489789247512817\n",
      "249 1.4884743690490723\n",
      "250 1.4302297830581665\n",
      "251 1.3744220733642578\n",
      "252 1.3205959796905518\n",
      "253 1.2689597606658936\n",
      "254 1.219290018081665\n",
      "255 1.1716923713684082\n",
      "256 1.1258469820022583\n",
      "257 1.0819668769836426\n",
      "258 1.0397037267684937\n",
      "259 0.9991722702980042\n",
      "260 0.9601643681526184\n",
      "261 0.9227458238601685\n",
      "262 0.8867437243461609\n",
      "263 0.8521658182144165\n",
      "264 0.8189404010772705\n",
      "265 0.787085771560669\n",
      "266 0.7563566565513611\n",
      "267 0.7269436120986938\n",
      "268 0.6986481547355652\n",
      "269 0.671517550945282\n",
      "270 0.6453801393508911\n",
      "271 0.6202819347381592\n",
      "272 0.5960772037506104\n",
      "273 0.572999894618988\n",
      "274 0.5506904721260071\n",
      "275 0.5292918682098389\n",
      "276 0.5086961984634399\n",
      "277 0.48897162079811096\n",
      "278 0.4700245261192322\n",
      "279 0.4517437815666199\n",
      "280 0.4341457188129425\n",
      "281 0.4173247218132019\n",
      "282 0.40115630626678467\n",
      "283 0.3855864107608795\n",
      "284 0.3706432580947876\n",
      "285 0.35624971985816956\n",
      "286 0.34243255853652954\n",
      "287 0.32915639877319336\n",
      "288 0.3163979649543762\n",
      "289 0.3041398227214813\n",
      "290 0.2923968434333801\n",
      "291 0.2811110019683838\n",
      "292 0.2702026069164276\n",
      "293 0.2597202658653259\n",
      "294 0.24968278408050537\n",
      "295 0.2399984449148178\n",
      "296 0.23071151971817017\n",
      "297 0.22182625532150269\n",
      "298 0.2132267951965332\n",
      "299 0.20500172674655914\n",
      "300 0.19709210097789764\n",
      "301 0.1894562691450119\n",
      "302 0.18216778337955475\n",
      "303 0.1750698983669281\n",
      "304 0.16829660534858704\n",
      "305 0.16182301938533783\n",
      "306 0.15557287633419037\n",
      "307 0.1495792120695114\n",
      "308 0.14383062720298767\n",
      "309 0.13830140233039856\n",
      "310 0.13296282291412354\n",
      "311 0.12784653902053833\n",
      "312 0.12290027737617493\n",
      "313 0.11816741526126862\n",
      "314 0.1135995164513588\n",
      "315 0.1092463955283165\n",
      "316 0.10501287132501602\n",
      "317 0.10097986459732056\n",
      "318 0.0970882996916771\n",
      "319 0.09336565434932709\n",
      "320 0.08975248038768768\n",
      "321 0.08630503714084625\n",
      "322 0.08297746628522873\n",
      "323 0.07980173081159592\n",
      "324 0.07671406865119934\n",
      "325 0.07376944273710251\n",
      "326 0.0709199532866478\n",
      "327 0.06819040328264236\n",
      "328 0.0655895322561264\n",
      "329 0.06308231502771378\n",
      "330 0.060654185712337494\n",
      "331 0.05833788588643074\n",
      "332 0.05607929080724716\n",
      "333 0.053928256034851074\n",
      "334 0.0518539659678936\n",
      "335 0.049860090017318726\n",
      "336 0.047956954687833786\n",
      "337 0.04613818600773811\n",
      "338 0.044369250535964966\n",
      "339 0.04267197102308273\n",
      "340 0.041027747094631195\n",
      "341 0.03946660831570625\n",
      "342 0.0379670076072216\n",
      "343 0.03650255501270294\n",
      "344 0.035126641392707825\n",
      "345 0.03377823159098625\n",
      "346 0.032486654818058014\n",
      "347 0.03123999759554863\n",
      "348 0.03006194345653057\n",
      "349 0.02890900894999504\n",
      "350 0.02780628390610218\n",
      "351 0.02674945630133152\n",
      "352 0.02574191987514496\n",
      "353 0.024759698659181595\n",
      "354 0.023816222324967384\n",
      "355 0.022905830293893814\n",
      "356 0.02203219383955002\n",
      "357 0.021200451999902725\n",
      "358 0.02039477229118347\n",
      "359 0.019631315022706985\n",
      "360 0.018886860460042953\n",
      "361 0.018167654052376747\n",
      "362 0.017479009926319122\n",
      "363 0.01682404987514019\n",
      "364 0.016190726310014725\n",
      "365 0.015573541633784771\n",
      "366 0.014985652640461922\n",
      "367 0.0144355995580554\n",
      "368 0.013893957249820232\n",
      "369 0.01336656417697668\n",
      "370 0.012871495448052883\n",
      "371 0.012387127615511417\n",
      "372 0.011919177137315273\n",
      "373 0.011477628722786903\n",
      "374 0.011053772643208504\n",
      "375 0.010639950633049011\n",
      "376 0.010254698805510998\n",
      "377 0.009870213456451893\n",
      "378 0.00950443372130394\n",
      "379 0.009151595644652843\n",
      "380 0.008814224041998386\n",
      "381 0.008492903783917427\n",
      "382 0.008179904893040657\n",
      "383 0.00787580106407404\n",
      "384 0.007590258494019508\n",
      "385 0.00731113413348794\n",
      "386 0.0070470888167619705\n",
      "387 0.006789690814912319\n",
      "388 0.006544806063175201\n",
      "389 0.006308513227850199\n",
      "390 0.0060807173140347\n",
      "391 0.00586416432633996\n",
      "392 0.005650372710078955\n",
      "393 0.005443799775093794\n",
      "394 0.0052531263791024685\n",
      "395 0.005068688653409481\n",
      "396 0.004890608601272106\n",
      "397 0.004716528579592705\n",
      "398 0.004550411365926266\n",
      "399 0.004392107482999563\n",
      "400 0.004240036942064762\n",
      "401 0.004087833687663078\n",
      "402 0.003948861267417669\n",
      "403 0.0038124769926071167\n",
      "404 0.003679586574435234\n",
      "405 0.003554430790245533\n",
      "406 0.003428333904594183\n",
      "407 0.003310907632112503\n",
      "408 0.0031995410099625587\n",
      "409 0.003090491285547614\n",
      "410 0.0029866087716072798\n",
      "411 0.002885716035962105\n",
      "412 0.002786742290481925\n",
      "413 0.002693623537197709\n",
      "414 0.002605138812214136\n",
      "415 0.002520500449463725\n",
      "416 0.00243568466976285\n",
      "417 0.0023552938364446163\n",
      "418 0.002279867883771658\n",
      "419 0.002206581411883235\n",
      "420 0.0021332839969545603\n",
      "421 0.0020676462445408106\n",
      "422 0.002001657849177718\n",
      "423 0.0019353971583768725\n",
      "424 0.0018753046169877052\n",
      "425 0.0018145968206226826\n",
      "426 0.0017574396915733814\n",
      "427 0.0017038420774042606\n",
      "428 0.001649891142733395\n",
      "429 0.0015978607116267085\n",
      "430 0.0015482683666050434\n",
      "431 0.0015015194658190012\n",
      "432 0.0014576041139662266\n",
      "433 0.0014121431158855557\n",
      "434 0.0013694934314116836\n",
      "435 0.001327230129390955\n",
      "436 0.0012894346145913005\n",
      "437 0.0012517502764239907\n",
      "438 0.0012149398680776358\n",
      "439 0.001179852639324963\n",
      "440 0.0011445656418800354\n",
      "441 0.0011132154613733292\n",
      "442 0.001082268776372075\n",
      "443 0.0010496573522686958\n",
      "444 0.0010199483949691057\n",
      "445 0.0009918989380821586\n",
      "446 0.0009645104873925447\n",
      "447 0.0009367262828163803\n",
      "448 0.0009112571133300662\n",
      "449 0.0008878477965481579\n",
      "450 0.0008613793179392815\n",
      "451 0.0008384619140997529\n",
      "452 0.000816094980109483\n",
      "453 0.0007941349176689982\n",
      "454 0.0007722029695287347\n",
      "455 0.0007522455416619778\n",
      "456 0.0007319439318962395\n",
      "457 0.0007121345261111856\n",
      "458 0.0006936191348358989\n",
      "459 0.0006762183038517833\n",
      "460 0.0006590171833522618\n",
      "461 0.0006421309080906212\n",
      "462 0.0006250509177334607\n",
      "463 0.0006096333963796496\n",
      "464 0.0005947101744823158\n",
      "465 0.0005792728043161333\n",
      "466 0.0005653743864968419\n",
      "467 0.0005505603039637208\n",
      "468 0.0005374783650040627\n",
      "469 0.000524580420460552\n",
      "470 0.0005119385896250606\n",
      "471 0.0005003711557947099\n",
      "472 0.000488357269205153\n",
      "473 0.00047675453242845833\n",
      "474 0.00046586792450398207\n",
      "475 0.0004558117361739278\n",
      "476 0.0004446943348739296\n",
      "477 0.00043416256085038185\n",
      "478 0.00042398303048685193\n",
      "479 0.0004147524887230247\n",
      "480 0.00040567939868196845\n",
      "481 0.00039601142634637654\n",
      "482 0.0003875751281157136\n",
      "483 0.00037925096694380045\n",
      "484 0.00037071449332870543\n",
      "485 0.00036256012390367687\n",
      "486 0.00035517362994141877\n",
      "487 0.0003468413488008082\n",
      "488 0.0003395438543520868\n",
      "489 0.00033200960024259984\n",
      "490 0.00032498655491508543\n",
      "491 0.00031813792884349823\n",
      "492 0.0003122588968835771\n",
      "493 0.0003055028500966728\n",
      "494 0.00029930099844932556\n",
      "495 0.00029380054911598563\n",
      "496 0.0002865967690013349\n",
      "497 0.0002807038545142859\n",
      "498 0.0002757403999567032\n",
      "499 0.0002696756273508072\n"
     ]
    }
   ],
   "source": [
    "for it in range(500): \n",
    "    # forward pass\n",
    "#     h=x.mm(w1) # N*H\n",
    "#     h_relu = h.clamp(min=0) # N*H\n",
    "#     y_pred = h_relu.mm(w2)  # N*D_out\n",
    "    \n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    #compute loss\n",
    "    loss = (y_pred - y).pow(2).sum()  # computation graph计算图\n",
    "    print(it,loss.item())\n",
    "    \n",
    "    # backword pass\n",
    "    # cumpute the gradient\n",
    "    # d loss/d w1 = 链式。。。\n",
    "#     grad_y_pred =2.0*(y_pred-y)\n",
    "#     grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "#     grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "#     grad_h = grad_h_relu.clone()\n",
    "#     grad_h[h<0]=0\n",
    "#     grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    # tensor所有运算都是计算图，为了让tensor不让计算图占内存\n",
    "    # update weights of w1 and w2\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate*w1.grad\n",
    "        w2 -= learning_rate*w2.grad\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch:nn\n",
    "- 这次使用pytorch中nn这个库来构建网络。\n",
    "- 用pytorch autograd来构建计算图和计算gradient\n",
    "- 然后python会自动计算gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 35557200.0\n",
      "1 36114676.0\n",
      "2 40089768.0\n",
      "3 38757912.0\n",
      "4 29164308.0\n",
      "5 16087516.0\n",
      "6 7443400.0\n",
      "7 3503010.25\n",
      "8 2000440.0\n",
      "9 1382073.75\n",
      "10 1072548.75\n",
      "11 878804.0\n",
      "12 738211.8125\n",
      "13 628176.125\n",
      "14 538972.0\n",
      "15 465244.0625\n",
      "16 403743.875\n",
      "17 351928.3125\n",
      "18 308266.5\n",
      "19 271093.21875\n",
      "20 239182.984375\n",
      "21 211652.765625\n",
      "22 187846.40625\n",
      "23 167189.71875\n",
      "24 149174.28125\n",
      "25 133415.9375\n",
      "26 119577.890625\n",
      "27 107382.65625\n",
      "28 96638.890625\n",
      "29 87172.5\n",
      "30 78764.828125\n",
      "31 71282.6796875\n",
      "32 64601.5859375\n",
      "33 58630.69140625\n",
      "34 53285.91796875\n",
      "35 48502.39453125\n",
      "36 44209.0703125\n",
      "37 40345.98046875\n",
      "38 36864.80078125\n",
      "39 33723.1640625\n",
      "40 30883.587890625\n",
      "41 28315.44921875\n",
      "42 25986.359375\n",
      "43 23871.50390625\n",
      "44 21948.615234375\n",
      "45 20198.72265625\n",
      "46 18605.1953125\n",
      "47 17151.55078125\n",
      "48 15824.62890625\n",
      "49 14611.1748046875\n",
      "50 13501.294921875\n",
      "51 12485.162109375\n",
      "52 11553.724609375\n",
      "53 10699.3271484375\n",
      "54 9914.69921875\n",
      "55 9193.8076171875\n",
      "56 8531.01953125\n",
      "57 7921.01025390625\n",
      "58 7359.09423828125\n",
      "59 6841.1552734375\n",
      "60 6363.5380859375\n",
      "61 5922.630859375\n",
      "62 5515.6474609375\n",
      "63 5139.490234375\n",
      "64 4791.49560546875\n",
      "65 4469.40234375\n",
      "66 4171.17138671875\n",
      "67 3894.97509765625\n",
      "68 3638.826416015625\n",
      "69 3401.2216796875\n",
      "70 3180.719482421875\n",
      "71 2975.98779296875\n",
      "72 2785.668701171875\n",
      "73 2608.77978515625\n",
      "74 2444.102294921875\n",
      "75 2290.822509765625\n",
      "76 2148.1220703125\n",
      "77 2015.254150390625\n",
      "78 1891.381591796875\n",
      "79 1775.8721923828125\n",
      "80 1668.045654296875\n",
      "81 1567.35498046875\n",
      "82 1473.302490234375\n",
      "83 1385.432861328125\n",
      "84 1303.3287353515625\n",
      "85 1226.46826171875\n",
      "86 1154.550537109375\n",
      "87 1087.2330322265625\n",
      "88 1024.1865234375\n",
      "89 965.1259765625\n",
      "90 909.7714233398438\n",
      "91 857.8699951171875\n",
      "92 809.1731567382812\n",
      "93 763.4868774414062\n",
      "94 720.6004028320312\n",
      "95 680.3477172851562\n",
      "96 642.539306640625\n",
      "97 607.0078125\n",
      "98 573.6134033203125\n",
      "99 542.2166137695312\n",
      "100 512.6625366210938\n",
      "101 484.83966064453125\n",
      "102 458.6463623046875\n",
      "103 433.98138427734375\n",
      "104 410.74188232421875\n",
      "105 388.85174560546875\n",
      "106 368.2133483886719\n",
      "107 348.74847412109375\n",
      "108 330.3856201171875\n",
      "109 313.0630187988281\n",
      "110 296.71923828125\n",
      "111 281.28448486328125\n",
      "112 266.7122802734375\n",
      "113 252.94683837890625\n",
      "114 239.94671630859375\n",
      "115 227.68392944335938\n",
      "116 216.09780883789062\n",
      "117 205.14334106445312\n",
      "118 194.78372192382812\n",
      "119 184.97994995117188\n",
      "120 175.70352172851562\n",
      "121 166.92401123046875\n",
      "122 158.6087188720703\n",
      "123 150.73471069335938\n",
      "124 143.27493286132812\n",
      "125 136.20693969726562\n",
      "126 129.50921630859375\n",
      "127 123.15922546386719\n",
      "128 117.13665008544922\n",
      "129 111.42949676513672\n",
      "130 106.01596069335938\n",
      "131 100.88206481933594\n",
      "132 96.00624084472656\n",
      "133 91.37928771972656\n",
      "134 86.98844909667969\n",
      "135 82.81843566894531\n",
      "136 78.85858154296875\n",
      "137 75.09770202636719\n",
      "138 71.52546691894531\n",
      "139 68.13142395019531\n",
      "140 64.90595245361328\n",
      "141 61.84165573120117\n",
      "142 58.927520751953125\n",
      "143 56.15857696533203\n",
      "144 53.52519226074219\n",
      "145 51.02019500732422\n",
      "146 48.63829803466797\n",
      "147 46.3726806640625\n",
      "148 44.21669387817383\n",
      "149 42.16523742675781\n",
      "150 40.2132568359375\n",
      "151 38.35490417480469\n",
      "152 36.585533142089844\n",
      "153 34.901123046875\n",
      "154 33.29789733886719\n",
      "155 31.770719528198242\n",
      "156 30.31597900390625\n",
      "157 28.93148422241211\n",
      "158 27.611175537109375\n",
      "159 26.354156494140625\n",
      "160 25.156007766723633\n",
      "161 24.01433563232422\n",
      "162 22.92612648010254\n",
      "163 21.889102935791016\n",
      "164 20.899625778198242\n",
      "165 19.957067489624023\n",
      "166 19.058076858520508\n",
      "167 18.20082664489746\n",
      "168 17.383861541748047\n",
      "169 16.6042423248291\n",
      "170 15.860578536987305\n",
      "171 15.151102066040039\n",
      "172 14.474506378173828\n",
      "173 13.828497886657715\n",
      "174 13.212543487548828\n",
      "175 12.624838829040527\n",
      "176 12.063718795776367\n",
      "177 11.528006553649902\n",
      "178 11.017318725585938\n",
      "179 10.529528617858887\n",
      "180 10.063823699951172\n",
      "181 9.619170188903809\n",
      "182 9.195024490356445\n",
      "183 8.789445877075195\n",
      "184 8.402180671691895\n",
      "185 8.032955169677734\n",
      "186 7.679870128631592\n",
      "187 7.342984676361084\n",
      "188 7.021016597747803\n",
      "189 6.713480472564697\n",
      "190 6.419739723205566\n",
      "191 6.138843059539795\n",
      "192 5.870436191558838\n",
      "193 5.614527702331543\n",
      "194 5.369808673858643\n",
      "195 5.135831832885742\n",
      "196 4.912315368652344\n",
      "197 4.69869327545166\n",
      "198 4.494457244873047\n",
      "199 4.299192905426025\n",
      "200 4.11279296875\n",
      "201 3.9343857765197754\n",
      "202 3.763878345489502\n",
      "203 3.600966215133667\n",
      "204 3.4454939365386963\n",
      "205 3.2964158058166504\n",
      "206 3.1540937423706055\n",
      "207 3.0179362297058105\n",
      "208 2.887838363647461\n",
      "209 2.7633676528930664\n",
      "210 2.6445393562316895\n",
      "211 2.530569553375244\n",
      "212 2.421823263168335\n",
      "213 2.31767201423645\n",
      "214 2.218062162399292\n",
      "215 2.12290096282959\n",
      "216 2.031822681427002\n",
      "217 1.944705843925476\n",
      "218 1.8614035844802856\n",
      "219 1.7816143035888672\n",
      "220 1.7053419351577759\n",
      "221 1.632489800453186\n",
      "222 1.5626472234725952\n",
      "223 1.4958925247192383\n",
      "224 1.4319690465927124\n",
      "225 1.3708076477050781\n",
      "226 1.312368392944336\n",
      "227 1.256360411643982\n",
      "228 1.2027966976165771\n",
      "229 1.151617169380188\n",
      "230 1.1025582551956177\n",
      "231 1.0556273460388184\n",
      "232 1.0107910633087158\n",
      "233 0.9677701592445374\n",
      "234 0.9267124533653259\n",
      "235 0.8872394561767578\n",
      "236 0.8496301770210266\n",
      "237 0.8135145902633667\n",
      "238 0.7789973020553589\n",
      "239 0.74596107006073\n",
      "240 0.7143294811248779\n",
      "241 0.6840799450874329\n",
      "242 0.6551282405853271\n",
      "243 0.6274417042732239\n",
      "244 0.6008303761482239\n",
      "245 0.5753730535507202\n",
      "246 0.5510892868041992\n",
      "247 0.5277974605560303\n",
      "248 0.5054960250854492\n",
      "249 0.48413193225860596\n",
      "250 0.4637024998664856\n",
      "251 0.4441260099411011\n",
      "252 0.42538654804229736\n",
      "253 0.40749526023864746\n",
      "254 0.3902868628501892\n",
      "255 0.373775452375412\n",
      "256 0.35801661014556885\n",
      "257 0.3429614305496216\n",
      "258 0.32853901386260986\n",
      "259 0.31468483805656433\n",
      "260 0.301510751247406\n",
      "261 0.2887810170650482\n",
      "262 0.27662786841392517\n",
      "263 0.2649981379508972\n",
      "264 0.2538830637931824\n",
      "265 0.2432330697774887\n",
      "266 0.23296602070331573\n",
      "267 0.2232111692428589\n",
      "268 0.2138138711452484\n",
      "269 0.20485134422779083\n",
      "270 0.19625692069530487\n",
      "271 0.18801403045654297\n",
      "272 0.18012219667434692\n",
      "273 0.1725669652223587\n",
      "274 0.16532176733016968\n",
      "275 0.15839993953704834\n",
      "276 0.15176451206207275\n",
      "277 0.14539605379104614\n",
      "278 0.13930414617061615\n",
      "279 0.1334623247385025\n",
      "280 0.12789049744606018\n",
      "281 0.12253344058990479\n",
      "282 0.11740346997976303\n",
      "283 0.11250389367341995\n",
      "284 0.107809878885746\n",
      "285 0.10329329967498779\n",
      "286 0.09898921102285385\n",
      "287 0.09485381096601486\n",
      "288 0.09088975191116333\n",
      "289 0.08709781616926193\n",
      "290 0.08345812559127808\n",
      "291 0.07999619096517563\n",
      "292 0.07664996385574341\n",
      "293 0.07345253974199295\n",
      "294 0.07037749141454697\n",
      "295 0.06745482236146927\n",
      "296 0.06464213877916336\n",
      "297 0.06194668635725975\n",
      "298 0.05935585871338844\n",
      "299 0.05689553916454315\n",
      "300 0.054531484842300415\n",
      "301 0.05226164311170578\n",
      "302 0.050086185336112976\n",
      "303 0.047990839928388596\n",
      "304 0.046013057231903076\n",
      "305 0.044093575328588486\n",
      "306 0.04225107654929161\n",
      "307 0.04050430655479431\n",
      "308 0.038812704384326935\n",
      "309 0.03721348196268082\n",
      "310 0.03566263988614082\n",
      "311 0.0341869555413723\n",
      "312 0.03276630491018295\n",
      "313 0.03141104057431221\n",
      "314 0.030119266360998154\n",
      "315 0.028867855668067932\n",
      "316 0.02767876908183098\n",
      "317 0.02654052898287773\n",
      "318 0.025442885234951973\n",
      "319 0.024384312331676483\n",
      "320 0.023383729159832\n",
      "321 0.02242320589721203\n",
      "322 0.021504782140254974\n",
      "323 0.020609360188245773\n",
      "324 0.0197611004114151\n",
      "325 0.018948709592223167\n",
      "326 0.018175963312387466\n",
      "327 0.017431791871786118\n",
      "328 0.01670696772634983\n",
      "329 0.016025161370635033\n",
      "330 0.015366114675998688\n",
      "331 0.014741920866072178\n",
      "332 0.014141466468572617\n",
      "333 0.01357048749923706\n",
      "334 0.013021288439631462\n",
      "335 0.012493525631725788\n",
      "336 0.011987708508968353\n",
      "337 0.011504069902002811\n",
      "338 0.01103959884494543\n",
      "339 0.010590103454887867\n",
      "340 0.010163038037717342\n",
      "341 0.009753121063113213\n",
      "342 0.009362705983221531\n",
      "343 0.008985272608697414\n",
      "344 0.008626297116279602\n",
      "345 0.008288717828691006\n",
      "346 0.007952646352350712\n",
      "347 0.00763054471462965\n",
      "348 0.007329793646931648\n",
      "349 0.0070422193966805935\n",
      "350 0.006762823089957237\n",
      "351 0.006494060158729553\n",
      "352 0.006240186281502247\n",
      "353 0.005994588136672974\n",
      "354 0.005757261998951435\n",
      "355 0.005531500093638897\n",
      "356 0.005314648151397705\n",
      "357 0.0051085674203932285\n",
      "358 0.004912861622869968\n",
      "359 0.004722815938293934\n",
      "360 0.004539807327091694\n",
      "361 0.004367008339613676\n",
      "362 0.004200381692498922\n",
      "363 0.00403972715139389\n",
      "364 0.0038841308560222387\n",
      "365 0.003735964884981513\n",
      "366 0.003595499088987708\n",
      "367 0.003457051469013095\n",
      "368 0.003331259358674288\n",
      "369 0.0032067743595689535\n",
      "370 0.0030913790687918663\n",
      "371 0.0029763802886009216\n",
      "372 0.0028648534789681435\n",
      "373 0.002759514842182398\n",
      "374 0.0026572986971586943\n",
      "375 0.0025611859746277332\n",
      "376 0.0024693934246897697\n",
      "377 0.002378859557211399\n",
      "378 0.0022971227299422026\n",
      "379 0.002213909290730953\n",
      "380 0.002137742005288601\n",
      "381 0.0020598238334059715\n",
      "382 0.001987135037779808\n",
      "383 0.0019177858484908938\n",
      "384 0.0018512422684580088\n",
      "385 0.0017853935714811087\n",
      "386 0.0017250091768801212\n",
      "387 0.0016654180362820625\n",
      "388 0.0016087931580841541\n",
      "389 0.0015568607486784458\n",
      "390 0.0015009220223873854\n",
      "391 0.0014506103470921516\n",
      "392 0.001402449095621705\n",
      "393 0.0013559020590037107\n",
      "394 0.001310824416577816\n",
      "395 0.0012683260720223188\n",
      "396 0.0012270163279026747\n",
      "397 0.001186641282401979\n",
      "398 0.0011492252815514803\n",
      "399 0.0011112343054264784\n",
      "400 0.001077571650967002\n",
      "401 0.0010435665026307106\n",
      "402 0.0010098800994455814\n",
      "403 0.000977628631517291\n",
      "404 0.0009493918623775244\n",
      "405 0.0009207352995872498\n",
      "406 0.0008917013183236122\n",
      "407 0.0008647024515084922\n",
      "408 0.0008383372914977372\n",
      "409 0.0008129716734401882\n",
      "410 0.0007877495372667909\n",
      "411 0.0007649800390936434\n",
      "412 0.000741641444619745\n",
      "413 0.000718810420949012\n",
      "414 0.0006982340128161013\n",
      "415 0.000677558418828994\n",
      "416 0.0006583462236449122\n",
      "417 0.0006400488782674074\n",
      "418 0.0006214690511114895\n",
      "419 0.0006036905688233674\n",
      "420 0.0005866335704922676\n",
      "421 0.0005699832690879703\n",
      "422 0.000555465288925916\n",
      "423 0.0005393977044150233\n",
      "424 0.0005237769219093025\n",
      "425 0.000509734614752233\n",
      "426 0.0004958827048540115\n",
      "427 0.0004830657853744924\n",
      "428 0.0004693788941949606\n",
      "429 0.0004571257159113884\n",
      "430 0.0004449698026292026\n",
      "431 0.00043418791028670967\n",
      "432 0.0004227464960422367\n",
      "433 0.0004114957991987467\n",
      "434 0.00040121644269675016\n",
      "435 0.00039035012014210224\n",
      "436 0.00038070048321969807\n",
      "437 0.00037116630119271576\n",
      "438 0.00036201291368342936\n",
      "439 0.0003519894671626389\n",
      "440 0.00034401268931105733\n",
      "441 0.0003355015651322901\n",
      "442 0.0003276455099694431\n",
      "443 0.0003195119497831911\n",
      "444 0.00031163450330495834\n",
      "445 0.0003040662850253284\n",
      "446 0.0002971958601847291\n",
      "447 0.00029032406746409833\n",
      "448 0.0002833871403709054\n",
      "449 0.0002761536161415279\n",
      "450 0.000269698619376868\n",
      "451 0.00026375986635684967\n",
      "452 0.0002578192506916821\n",
      "453 0.000252456811722368\n",
      "454 0.00024672242579981685\n",
      "455 0.00024105521151795983\n",
      "456 0.00023512802727054805\n",
      "457 0.0002302879875060171\n",
      "458 0.00022564613027498126\n",
      "459 0.0002206124336225912\n",
      "460 0.00021594975260086358\n",
      "461 0.00021160596224945039\n",
      "462 0.00020751934789586812\n",
      "463 0.00020316173322498798\n",
      "464 0.00019879528554156423\n",
      "465 0.00019413149857427925\n",
      "466 0.0001896816975204274\n",
      "467 0.00018555234419181943\n",
      "468 0.0001820043835323304\n",
      "469 0.0001784362393664196\n",
      "470 0.0001750306400936097\n",
      "471 0.00017157952242996544\n",
      "472 0.00016817783762235194\n",
      "473 0.00016487058019265532\n",
      "474 0.00016159196093212813\n",
      "475 0.0001587625010870397\n",
      "476 0.00015568554226774722\n",
      "477 0.0001526637061033398\n",
      "478 0.00014962283603381366\n",
      "479 0.0001466653193347156\n",
      "480 0.00014417202328331769\n",
      "481 0.00014076245133765042\n",
      "482 0.00013861211482435465\n",
      "483 0.0001360647293040529\n",
      "484 0.00013387123181018978\n",
      "485 0.00013143250544089824\n",
      "486 0.00012887976481579244\n",
      "487 0.00012603559298440814\n",
      "488 0.00012407655594870448\n",
      "489 0.0001219596597366035\n",
      "490 0.00011983246804447845\n",
      "491 0.00011767919932026416\n",
      "492 0.00011590259236982092\n",
      "493 0.00011326235835440457\n",
      "494 0.0001112185709644109\n",
      "495 0.00010942592052742839\n",
      "496 0.0001073651437764056\n",
      "497 0.00010551018931437284\n",
      "498 0.00010409075184725225\n",
      "499 0.00010241645213682204\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "N=64 # 64个输入\n",
    "D_in=1000 #输入1000维\n",
    "H=100 # 中间层 100维\n",
    "D_out=10 #输出10维\n",
    "\n",
    "# 随机创建一些训练数据\n",
    "x = torch.randn(N,D_in).to('cuda:0')\n",
    "y = torch.randn(N,D_out).to('cuda:0')\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in,H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H,D_out),\n",
    ")\n",
    "\n",
    "torch.nn.init.normal_(model[0].weight)#经验\n",
    "torch.nn.init.normal_(model[2].weight)\n",
    "\n",
    "model = model.to('cuda:0')\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate=1e-6\n",
    "\n",
    "for it in range(500): \n",
    "    # forward pass\n",
    "#     h=x.mm(w1) # N*H\n",
    "#     h_relu = h.clamp(min=0) # N*H\n",
    "#     y_pred = h_relu.mm(w2)  # N*D_out\n",
    "    \n",
    "    y_pred = model(x) # model.forward()\n",
    "    \n",
    "    #compute loss\n",
    "    loss = loss_fn(y_pred,y)  # computation graph计算图\n",
    "    print(it,loss.item())\n",
    "    \n",
    "    # backword pass\n",
    "    # cumpute the gradient\n",
    "    # d loss/d w1 = 链式。。。\n",
    "#     grad_y_pred =2.0*(y_pred-y)\n",
    "#     grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "#     grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "#     grad_h = grad_h_relu.clone()\n",
    "#     grad_h[h<0]=0\n",
    "#     grad_w1 = x.t().mm(grad_h)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # tensor所有运算都是计算图，为了让tensor不让计算图占内存\n",
    "    # update weights of w1 and w2\n",
    "    with torch.no_grad():  # param(tensor,grad)\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate*param.grad\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 5.9931e-03,  2.5495e-02, -2.3114e-02,  ..., -2.5103e-02,\n",
       "         -1.1510e-02,  3.1857e-02],\n",
       "        [ 1.2912e-02, -1.4467e-02,  6.9403e-03,  ..., -6.1057e-03,\n",
       "         -1.3359e-02, -2.8196e-02],\n",
       "        [ 2.5674e-02,  1.0794e-02,  3.3988e-03,  ...,  2.3447e-02,\n",
       "         -2.1006e-02, -2.5804e-02],\n",
       "        ...,\n",
       "        [ 9.1671e-03, -1.6116e-02, -9.6578e-03,  ...,  1.6271e-02,\n",
       "         -2.5629e-02,  7.0179e-03],\n",
       "        [ 7.1602e-03, -5.6875e-03, -6.9846e-03,  ...,  2.7993e-02,\n",
       "          1.3654e-02,  1.6649e-02],\n",
       "        [-1.4346e-02, -2.4755e-02, -2.2672e-05,  ..., -2.2645e-02,\n",
       "         -2.0813e-02, -7.6156e-03]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch:optim\n",
    "- 这一次不在手动更新模型的weights，而是使用optim这个包来帮助我们更新参数。\n",
    "- optim这个package提供了各种不同的模型优化方法：SGD+momentum，RMSProp，adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 703.599609375\n",
      "1 686.0562133789062\n",
      "2 669.0096435546875\n",
      "3 652.470947265625\n",
      "4 636.4136962890625\n",
      "5 620.8534545898438\n",
      "6 605.72119140625\n",
      "7 591.044189453125\n",
      "8 576.7297973632812\n",
      "9 562.7486572265625\n",
      "10 549.1608276367188\n",
      "11 535.995361328125\n",
      "12 523.24072265625\n",
      "13 510.86370849609375\n",
      "14 498.89849853515625\n",
      "15 487.27288818359375\n",
      "16 475.99774169921875\n",
      "17 465.00640869140625\n",
      "18 454.2627258300781\n",
      "19 443.79815673828125\n",
      "20 433.630615234375\n",
      "21 423.7554931640625\n",
      "22 414.1490478515625\n",
      "23 404.7812805175781\n",
      "24 395.62896728515625\n",
      "25 386.7212219238281\n",
      "26 378.0419921875\n",
      "27 369.5470275878906\n",
      "28 361.2042236328125\n",
      "29 353.05023193359375\n",
      "30 345.07452392578125\n",
      "31 337.2701416015625\n",
      "32 329.6502990722656\n",
      "33 322.21197509765625\n",
      "34 314.94085693359375\n",
      "35 307.826171875\n",
      "36 300.8843994140625\n",
      "37 294.0935974121094\n",
      "38 287.41400146484375\n",
      "39 280.84564208984375\n",
      "40 274.42889404296875\n",
      "41 268.16162109375\n",
      "42 262.02081298828125\n",
      "43 255.996337890625\n",
      "44 250.08665466308594\n",
      "45 244.3001708984375\n",
      "46 238.62835693359375\n",
      "47 233.07894897460938\n",
      "48 227.6645965576172\n",
      "49 222.3568115234375\n",
      "50 217.15634155273438\n",
      "51 212.05332946777344\n",
      "52 207.0599365234375\n",
      "53 202.17898559570312\n",
      "54 197.38902282714844\n",
      "55 192.70204162597656\n",
      "56 188.10382080078125\n",
      "57 183.61285400390625\n",
      "58 179.2142333984375\n",
      "59 174.9033660888672\n",
      "60 170.68157958984375\n",
      "61 166.54290771484375\n",
      "62 162.48495483398438\n",
      "63 158.52377319335938\n",
      "64 154.64352416992188\n",
      "65 150.83485412597656\n",
      "66 147.10325622558594\n",
      "67 143.46153259277344\n",
      "68 139.88555908203125\n",
      "69 136.37301635742188\n",
      "70 132.91482543945312\n",
      "71 129.53273010253906\n",
      "72 126.21965026855469\n",
      "73 122.96562957763672\n",
      "74 119.77813720703125\n",
      "75 116.66118621826172\n",
      "76 113.60557556152344\n",
      "77 110.61784362792969\n",
      "78 107.69107055664062\n",
      "79 104.8289794921875\n",
      "80 102.02439880371094\n",
      "81 99.27740478515625\n",
      "82 96.59256744384766\n",
      "83 93.97040557861328\n",
      "84 91.40231323242188\n",
      "85 88.8901138305664\n",
      "86 86.43743133544922\n",
      "87 84.03759765625\n",
      "88 81.69046783447266\n",
      "89 79.39436340332031\n",
      "90 77.15234375\n",
      "91 74.96185302734375\n",
      "92 72.82328796386719\n",
      "93 70.72256469726562\n",
      "94 68.67390441894531\n",
      "95 66.66842651367188\n",
      "96 64.7139892578125\n",
      "97 62.807891845703125\n",
      "98 60.94756317138672\n",
      "99 59.13082504272461\n",
      "100 57.3583984375\n",
      "101 55.629539489746094\n",
      "102 53.94347381591797\n",
      "103 52.30060958862305\n",
      "104 50.69941329956055\n",
      "105 49.13707733154297\n",
      "106 47.619075775146484\n",
      "107 46.13993835449219\n",
      "108 44.70093536376953\n",
      "109 43.298526763916016\n",
      "110 41.933563232421875\n",
      "111 40.60541915893555\n",
      "112 39.31508255004883\n",
      "113 38.060882568359375\n",
      "114 36.841026306152344\n",
      "115 35.654693603515625\n",
      "116 34.50209045410156\n",
      "117 33.38285827636719\n",
      "118 32.2957649230957\n",
      "119 31.23867416381836\n",
      "120 30.212032318115234\n",
      "121 29.214920043945312\n",
      "122 28.24597930908203\n",
      "123 27.304706573486328\n",
      "124 26.39202880859375\n",
      "125 25.505718231201172\n",
      "126 24.64613151550293\n",
      "127 23.81309700012207\n",
      "128 23.007081985473633\n",
      "129 22.225322723388672\n",
      "130 21.468257904052734\n",
      "131 20.734296798706055\n",
      "132 20.02395248413086\n",
      "133 19.336997985839844\n",
      "134 18.670879364013672\n",
      "135 18.025901794433594\n",
      "136 17.401491165161133\n",
      "137 16.797103881835938\n",
      "138 16.212554931640625\n",
      "139 15.646821975708008\n",
      "140 15.10000228881836\n",
      "141 14.571355819702148\n",
      "142 14.060840606689453\n",
      "143 13.568429946899414\n",
      "144 13.092345237731934\n",
      "145 12.632834434509277\n",
      "146 12.189019203186035\n",
      "147 11.760662078857422\n",
      "148 11.34691047668457\n",
      "149 10.9472074508667\n",
      "150 10.560993194580078\n",
      "151 10.18906021118164\n",
      "152 9.829307556152344\n",
      "153 9.482110977172852\n",
      "154 9.146966934204102\n",
      "155 8.823495864868164\n",
      "156 8.5111083984375\n",
      "157 8.210186004638672\n",
      "158 7.919227123260498\n",
      "159 7.638523101806641\n",
      "160 7.3675432205200195\n",
      "161 7.105979919433594\n",
      "162 6.853006839752197\n",
      "163 6.609046459197998\n",
      "164 6.373671054840088\n",
      "165 6.14652156829834\n",
      "166 5.926923751831055\n",
      "167 5.715181350708008\n",
      "168 5.510316848754883\n",
      "169 5.312456130981445\n",
      "170 5.121304035186768\n",
      "171 4.936990737915039\n",
      "172 4.759210586547852\n",
      "173 4.5873589515686035\n",
      "174 4.421532154083252\n",
      "175 4.261600017547607\n",
      "176 4.107306480407715\n",
      "177 3.9584856033325195\n",
      "178 3.814776659011841\n",
      "179 3.6762681007385254\n",
      "180 3.5425503253936768\n",
      "181 3.413795232772827\n",
      "182 3.2893099784851074\n",
      "183 3.1694681644439697\n",
      "184 3.0538806915283203\n",
      "185 2.9425320625305176\n",
      "186 2.835207462310791\n",
      "187 2.7318131923675537\n",
      "188 2.6321780681610107\n",
      "189 2.5362131595611572\n",
      "190 2.4437198638916016\n",
      "191 2.3545117378234863\n",
      "192 2.2685883045196533\n",
      "193 2.1858794689178467\n",
      "194 2.106015682220459\n",
      "195 2.029169797897339\n",
      "196 1.9550689458847046\n",
      "197 1.8836487531661987\n",
      "198 1.8148566484451294\n",
      "199 1.7485824823379517\n",
      "200 1.6846593618392944\n",
      "201 1.6230442523956299\n",
      "202 1.5636759996414185\n",
      "203 1.5064808130264282\n",
      "204 1.4512889385223389\n",
      "205 1.3981698751449585\n",
      "206 1.3469061851501465\n",
      "207 1.297522783279419\n",
      "208 1.2499996423721313\n",
      "209 1.2040711641311646\n",
      "210 1.159893274307251\n",
      "211 1.1172960996627808\n",
      "212 1.0762619972229004\n",
      "213 1.036687970161438\n",
      "214 0.998537540435791\n",
      "215 0.9618414044380188\n",
      "216 0.9264465570449829\n",
      "217 0.8923085927963257\n",
      "218 0.8594399690628052\n",
      "219 0.8277524709701538\n",
      "220 0.7972205281257629\n",
      "221 0.7677930593490601\n",
      "222 0.7394387722015381\n",
      "223 0.7121392488479614\n",
      "224 0.6857762932777405\n",
      "225 0.660391628742218\n",
      "226 0.635928750038147\n",
      "227 0.6123474836349487\n",
      "228 0.5896208882331848\n",
      "229 0.567740797996521\n",
      "230 0.5466164350509644\n",
      "231 0.5262776017189026\n",
      "232 0.5066691637039185\n",
      "233 0.4877833127975464\n",
      "234 0.46958720684051514\n",
      "235 0.4520255923271179\n",
      "236 0.4351136088371277\n",
      "237 0.41881704330444336\n",
      "238 0.40311291813850403\n",
      "239 0.3879670202732086\n",
      "240 0.3733769655227661\n",
      "241 0.35936373472213745\n",
      "242 0.345846027135849\n",
      "243 0.3328295350074768\n",
      "244 0.3202851712703705\n",
      "245 0.30820032954216003\n",
      "246 0.2965494990348816\n",
      "247 0.2853212058544159\n",
      "248 0.2745102047920227\n",
      "249 0.2640843093395233\n",
      "250 0.25403308868408203\n",
      "251 0.24435847997665405\n",
      "252 0.2350301593542099\n",
      "253 0.22604486346244812\n",
      "254 0.21738557517528534\n",
      "255 0.20904065668582916\n",
      "256 0.20101022720336914\n",
      "257 0.19327031075954437\n",
      "258 0.18581804633140564\n",
      "259 0.17863807082176208\n",
      "260 0.17171940207481384\n",
      "261 0.16505330801010132\n",
      "262 0.1586327701807022\n",
      "263 0.15244877338409424\n",
      "264 0.14649143815040588\n",
      "265 0.1407555341720581\n",
      "266 0.13523277640342712\n",
      "267 0.12991875410079956\n",
      "268 0.12480563670396805\n",
      "269 0.11987990885972977\n",
      "270 0.11514055728912354\n",
      "271 0.11057263612747192\n",
      "272 0.10617823898792267\n",
      "273 0.10195238888263702\n",
      "274 0.09787556529045105\n",
      "275 0.09395618736743927\n",
      "276 0.09018334001302719\n",
      "277 0.08655218780040741\n",
      "278 0.08305861055850983\n",
      "279 0.07969895005226135\n",
      "280 0.07646440714597702\n",
      "281 0.07335575670003891\n",
      "282 0.07036830484867096\n",
      "283 0.06749501824378967\n",
      "284 0.06473040580749512\n",
      "285 0.06207277625799179\n",
      "286 0.05951699987053871\n",
      "287 0.057059451937675476\n",
      "288 0.0546962134540081\n",
      "289 0.0524248443543911\n",
      "290 0.05024202913045883\n",
      "291 0.04814339056611061\n",
      "292 0.046127695590257645\n",
      "293 0.044188857078552246\n",
      "294 0.04232705757021904\n",
      "295 0.040538445115089417\n",
      "296 0.03882025182247162\n",
      "297 0.03717167675495148\n",
      "298 0.03558524698019028\n",
      "299 0.03406348079442978\n",
      "300 0.0326022244989872\n",
      "301 0.031199656426906586\n",
      "302 0.029853153973817825\n",
      "303 0.028560392558574677\n",
      "304 0.027320269495248795\n",
      "305 0.02613033726811409\n",
      "306 0.024988675490021706\n",
      "307 0.023893270641565323\n",
      "308 0.022842606529593468\n",
      "309 0.021835099905729294\n",
      "310 0.020868973806500435\n",
      "311 0.01994282193481922\n",
      "312 0.019054951146245003\n",
      "313 0.01820415072143078\n",
      "314 0.017388934269547462\n",
      "315 0.016607029363512993\n",
      "316 0.01585841178894043\n",
      "317 0.015141273848712444\n",
      "318 0.014454377815127373\n",
      "319 0.013796621933579445\n",
      "320 0.013166964054107666\n",
      "321 0.012563726864755154\n",
      "322 0.011986605823040009\n",
      "323 0.011434035375714302\n",
      "324 0.01090548187494278\n",
      "325 0.010399800725281239\n",
      "326 0.00991598330438137\n",
      "327 0.009453262202441692\n",
      "328 0.009010634385049343\n",
      "329 0.008587545715272427\n",
      "330 0.008183056488633156\n",
      "331 0.007796409539878368\n",
      "332 0.007426864467561245\n",
      "333 0.007073850370943546\n",
      "334 0.006736449431627989\n",
      "335 0.006414168514311314\n",
      "336 0.0061064292676746845\n",
      "337 0.00581252621486783\n",
      "338 0.005531931761652231\n",
      "339 0.005264043342322111\n",
      "340 0.005008345935493708\n",
      "341 0.004764398094266653\n",
      "342 0.004531555809080601\n",
      "343 0.004309475421905518\n",
      "344 0.004097677301615477\n",
      "345 0.003895635949447751\n",
      "346 0.0037030016537755728\n",
      "347 0.0035193818621337414\n",
      "348 0.0033443078864365816\n",
      "349 0.003177459817379713\n",
      "350 0.0030184423085302114\n",
      "351 0.0028669731691479683\n",
      "352 0.002722685458138585\n",
      "353 0.002585237380117178\n",
      "354 0.0024543404579162598\n",
      "355 0.002329726004973054\n",
      "356 0.0022110852878540754\n",
      "357 0.002098171506077051\n",
      "358 0.001990686170756817\n",
      "359 0.001888429163955152\n",
      "360 0.0017911532195284963\n",
      "361 0.0016987128183245659\n",
      "362 0.0016106367111206055\n",
      "363 0.0015269685536623001\n",
      "364 0.0014474047347903252\n",
      "365 0.0013717857655137777\n",
      "366 0.0012999115278944373\n",
      "367 0.0012316082138568163\n",
      "368 0.0011667131911963224\n",
      "369 0.0011050564935430884\n",
      "370 0.001046490389853716\n",
      "371 0.0009908864740282297\n",
      "372 0.000938093347940594\n",
      "373 0.0008879811502993107\n",
      "374 0.0008403947576880455\n",
      "375 0.0007952414453029633\n",
      "376 0.0007523992680944502\n",
      "377 0.000711749424226582\n",
      "378 0.0006732054753229022\n",
      "379 0.0006366307497955859\n",
      "380 0.0006019584252499044\n",
      "381 0.0005690816906280816\n",
      "382 0.0005379114882089198\n",
      "383 0.0005083790747448802\n",
      "384 0.0004803817137144506\n",
      "385 0.00045387260615825653\n",
      "386 0.00042874086648225784\n",
      "387 0.0004049555864185095\n",
      "388 0.0003824114101007581\n",
      "389 0.0003610796411521733\n",
      "390 0.00034086935920640826\n",
      "391 0.00032175349770113826\n",
      "392 0.0003036564157810062\n",
      "393 0.0002865346032194793\n",
      "394 0.0002703384088817984\n",
      "395 0.0002550118661019951\n",
      "396 0.00024051936634350568\n",
      "397 0.0002268142270622775\n",
      "398 0.00021385344734881073\n",
      "399 0.0002016077924054116\n",
      "400 0.0001900249917525798\n",
      "401 0.00017908323206938803\n",
      "402 0.0001687504700385034\n",
      "403 0.00015898453420959413\n",
      "404 0.0001497565972385928\n",
      "405 0.00014104510773904622\n",
      "406 0.00013281939027365297\n",
      "407 0.00012505210179369897\n",
      "408 0.00011772131256293505\n",
      "409 0.00011080518743256107\n",
      "410 0.00010427922825329006\n",
      "411 9.812002826947719e-05\n",
      "412 9.230717842001468e-05\n",
      "413 8.683028136147186e-05\n",
      "414 8.166061161318794e-05\n",
      "415 7.678838301217183e-05\n",
      "416 7.219506369438022e-05\n",
      "417 6.786563608329743e-05\n",
      "418 6.378480611601844e-05\n",
      "419 5.9942100051557645e-05\n",
      "420 5.631941894534975e-05\n",
      "421 5.2907460485585034e-05\n",
      "422 4.9694546760292724e-05\n",
      "423 4.6670531446579844e-05\n",
      "424 4.3822641600854695e-05\n",
      "425 4.114032708457671e-05\n",
      "426 3.861847653752193e-05\n",
      "427 3.6243931390345097e-05\n",
      "428 3.400974674150348e-05\n",
      "429 3.190909410477616e-05\n",
      "430 2.9932496545370668e-05\n",
      "431 2.807400596793741e-05\n",
      "432 2.632620817166753e-05\n",
      "433 2.468246384523809e-05\n",
      "434 2.313903132744599e-05\n",
      "435 2.1689060304197483e-05\n",
      "436 2.0325140212662518e-05\n",
      "437 1.9043252905248664e-05\n",
      "438 1.7841284716269e-05\n",
      "439 1.6711626813048497e-05\n",
      "440 1.5649680790374987e-05\n",
      "441 1.4654648111900315e-05\n",
      "442 1.3719820344704203e-05\n",
      "443 1.2842399883084e-05\n",
      "444 1.2018622328469064e-05\n",
      "445 1.124668779084459e-05\n",
      "446 1.052148400049191e-05\n",
      "447 9.841416613198817e-06\n",
      "448 9.205226888298057e-06\n",
      "449 8.608041753177531e-06\n",
      "450 8.047160008572973e-06\n",
      "451 7.522193300246727e-06\n",
      "452 7.030371307337191e-06\n",
      "453 6.5694730437826365e-06\n",
      "454 6.138986918813316e-06\n",
      "455 5.733301350119291e-06\n",
      "456 5.355326720746234e-06\n",
      "457 5.0011267376248725e-06\n",
      "458 4.669168902182719e-06\n",
      "459 4.3593063310254365e-06\n",
      "460 4.068372618348803e-06\n",
      "461 3.796511236942024e-06\n",
      "462 3.5422617656877264e-06\n",
      "463 3.3040091693692375e-06\n",
      "464 3.0815947411610978e-06\n",
      "465 2.87384773400845e-06\n",
      "466 2.678911414477625e-06\n",
      "467 2.4973685412987834e-06\n",
      "468 2.32739012062666e-06\n",
      "469 2.168867467844393e-06\n",
      "470 2.0205595774314133e-06\n",
      "471 1.8825677443601307e-06\n",
      "472 1.7534173366584582e-06\n",
      "473 1.6329349818988703e-06\n",
      "474 1.5198780829450698e-06\n",
      "475 1.4148191667118226e-06\n",
      "476 1.3169355952413753e-06\n",
      "477 1.2252395436007646e-06\n",
      "478 1.1400094308555708e-06\n",
      "479 1.060298586708086e-06\n",
      "480 9.861989838100271e-07\n",
      "481 9.167874850390945e-07\n",
      "482 8.525416888005566e-07\n",
      "483 7.92373441527161e-07\n",
      "484 7.361426810348348e-07\n",
      "485 6.840825221843261e-07\n",
      "486 6.352763080030854e-07\n",
      "487 5.90027411817573e-07\n",
      "488 5.479033120536769e-07\n",
      "489 5.088410262032994e-07\n",
      "490 4.7224040145010804e-07\n",
      "491 4.3817888695230067e-07\n",
      "492 4.0652611232872005e-07\n",
      "493 3.7717336454079486e-07\n",
      "494 3.499004606055678e-07\n",
      "495 3.2448008369101444e-07\n",
      "496 3.009282636412536e-07\n",
      "497 2.789323332308413e-07\n",
      "498 2.585265121979319e-07\n",
      "499 2.3961143824635656e-07\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "N=64 # 64个输入\n",
    "D_in=1000 #输入1000维\n",
    "H=100 # 中间层 100维\n",
    "D_out=10 #输出10维\n",
    "\n",
    "\n",
    "# 随机创建一些训练数据\n",
    "x = torch.randn(N,D_in).to('cuda:0')\n",
    "y = torch.randn(N,D_out).to('cuda:0')\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in,H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H,D_out),\n",
    ")\n",
    "\n",
    "# torch.nn.init.normal_(model[0].weight)#经验 一些模型的初始化适合一些不适合\n",
    "# torch.nn.init.normal_(model[2].weight)\n",
    "\n",
    "model = model.to('cuda:0')\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "for it in range(500): \n",
    "    # forward pass\n",
    "#     h=x.mm(w1) # N*H\n",
    "#     h_relu = h.clamp(min=0) # N*H\n",
    "#     y_pred = h_relu.mm(w2)  # N*D_out\n",
    "    \n",
    "    y_pred = model(x) # model.forward()\n",
    "    \n",
    "    #compute loss\n",
    "    loss = loss_fn(y_pred,y)  # computation graph计算图\n",
    "    print(it,loss.item())\n",
    "    \n",
    "    # backword pass\n",
    "    # cumpute the gradient\n",
    "    # d loss/d w1 = 链式。。。\n",
    "#     grad_y_pred =2.0*(y_pred-y)\n",
    "#     grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "#     grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "#     grad_h = grad_h_relu.clone()\n",
    "#     grad_h[h<0]=0\n",
    "#     grad_w1 = x.t().mm(grad_h)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # update weights of w1 and w2\n",
    "    optimizer.step()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1a0762e83cdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m#     y_pred = h_relu.mm(w2)  # N*D_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# model.forward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m#compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-1a0762e83cdb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtens_ops\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtens_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtens_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "N=64 # 64个输入\n",
    "D_in=1000 #输入1000维\n",
    "H=100 # 中间层 100维\n",
    "D_out=10 #输出10维\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self,D_in,H,D_out):\n",
    "        super(TwoLayerNet,self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in,H,bias=False)\n",
    "        self.linear2 = torch.nn.Linear(H,D_out,bias=False)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        y_pred = self.linear2(self.linear1(1).clamp(min=0))\n",
    "        return y_pred\n",
    "    \n",
    "model=TwoLayerNet(D_in,H,D_out)\n",
    "\n",
    "# 随机创建一些训练数据\n",
    "x = torch.randn(N,D_in).to('cuda:0')\n",
    "y = torch.randn(N,D_out).to('cuda:0')\n",
    "\n",
    "\n",
    "\n",
    "# torch.nn.init.normal_(model[0].weight)#经验 一些模型的初始化适合一些不适合\n",
    "# torch.nn.init.normal_(model[2].weight)\n",
    "\n",
    "model = model.to('cuda:0')\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "for it in range(500): \n",
    "    # forward pass\n",
    "#     h=x.mm(w1) # N*H\n",
    "#     h_relu = h.clamp(min=0) # N*H\n",
    "#     y_pred = h_relu.mm(w2)  # N*D_out\n",
    "    \n",
    "    y_pred = model(x) # model.forward()\n",
    "    \n",
    "    #compute loss\n",
    "    loss = loss_fn(y_pred,y)  # computation graph计算图\n",
    "    print(it,loss.item())\n",
    "    \n",
    "    # backword pass\n",
    "    # cumpute the gradient\n",
    "    # d loss/d w1 = 链式。。。\n",
    "#     grad_y_pred =2.0*(y_pred-y)\n",
    "#     grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "#     grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "#     grad_h = grad_h_relu.clone()\n",
    "#     grad_h[h<0]=0\n",
    "#     grad_w1 = x.t().mm(grad_h)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # update weights of w1 and w2\n",
    "    optimizer.step()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
