{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from VGGModel.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from VGGModel import vgg\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_DEVICE_ORDER'] = \"PCI_BUS_ID\"  # os.environ[“CUDA_DEVICE_ORDER”] = “PCI_BUS_ID” # 按照PCI_BUS_ID顺序从0开始排列GPU设备\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'        # 设置当前使用的GPU设备仅为0号设备  设备名称为'/gpu:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "logical_gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu,True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "        exit(-1)\n",
    "logical_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = os.path.abspath(os.path.join(os.getcwd(),\"../../datasets\"))\n",
    "image_path = data_root+\"/flower_data/\"\n",
    "train_dir = image_path+'train'\n",
    "validation_dir = image_path+'val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_height = 224\n",
    "im_width = 224\n",
    "batch_size = 32\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('save_weights'):\n",
    "    os.makedirs(\"save_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'dandelion', 1: 'daisy', 2: 'roses', 3: 'tulips', 4: 'sunflowers'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class dict\n",
    "data_class = [cla for cla in os.listdir(train_dir) if '.txt' not in cla]\n",
    "class_num = len(data_class)\n",
    "class_dict = dict((value,index) for index,value in enumerate(data_class))\n",
    "inverse_dict = dict((value,key) for key,value in class_dict.items())\n",
    "inverse_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train images list\n",
    "train_image_list = glob.glob(train_dir+\"/*/*.jpg\")\n",
    "random.shuffle(train_image_list)\n",
    "train_num = len(train_image_list)\n",
    "train_label_list = [class_dict[path.split(os.path.sep)[-2]] for path in train_image_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load validation images list\n",
    "val_image_list = glob.glob(validation_dir+\"/*/*.jpg\")\n",
    "random.shuffle(val_image_list)\n",
    "val_num = len(val_image_list)\n",
    "val_label_list = [class_dict[path.split(os.path.sep)[-2]] for path in val_image_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_path(img_path,label):\n",
    "    label = tf.one_hot(label,depth=class_num)\n",
    "    image = tf.io.read_file(img_path)\n",
    "    image = tf.image.decode_jpeg(image)\n",
    "    image = tf.image.convert_image_dtype(image,tf.float32)\n",
    "    image = tf.image.resize(image,[im_height,im_width])\n",
    "    return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size_per_replica = 32\n",
    "# # Global batch size\n",
    "# GLOBAL_BATCH_SIZE = batch_size_per_replica * strategy.num_replicas_in_sync\n",
    "# # Buffer size for data loader\n",
    "# BUFFER_SIZE = batch_size_per_replica * strategy.num_replicas_in_sync * 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train dataset\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with strategy.scope():\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_image_list,train_label_list))\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=train_num)\\\n",
    "                                .map(process_path,num_parallel_calls=AUTOTUNE)\\\n",
    "                                .repeat().batch(batch_size).prefetch(AUTOTUNE)\n",
    "    \n",
    "    \n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((val_image_list,val_label_list))\n",
    "    val_dataset = val_dataset.map(process_path,num_parallel_calls=AUTOTUNE).repeat().batch(batch_size)\n",
    "    \n",
    "    train_dataset_distribute = strategy.experimental_distribute_dataset(train_dataset)\n",
    "    val_dataset_distribute = strategy.experimental_distribute_dataset(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "feature (Sequential)         (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2048)              51382272  \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 2048)              4196352   \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 5)                 10245     \n",
      "_________________________________________________________________\n",
      "softmax_4 (Softmax)          (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 70,303,557\n",
      "Trainable params: 70,303,557\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Please use `tf.keras.losses.Reduction.SUM` or `tf.keras.losses.Reduction.NONE` for loss reduction when losses are used with `tf.distribute.Strategy` outside of the built-in training loops. You can implement `tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE` using global batch size like:\n```\nwith strategy.scope():\n    loss_obj = tf.keras.losses.CategoricalCrossentropy(reduction=tf.keras.losses.reduction.NONE)\n....\n    loss = tf.reduce_sum(loss_obj(labels, predictions)) * (1. / global_batch_size)\n```\nPlease see https://www.tensorflow.org/alpha/tutorials/distribute/training_loops for more details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-dc415a2a0a9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtrain_step_num\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-dc415a2a0a9d>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(images, labels)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/losses.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    126\u001b[0m       \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m       return losses_utils.compute_weighted_loss(\n\u001b[0;32m--> 128\u001b[0;31m           losses, sample_weight, reduction=self._get_reduction())\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/losses.py\u001b[0m in \u001b[0;36m_get_reduction\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    160\u001b[0m         self.reduction == losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE):\n\u001b[1;32m    161\u001b[0m       raise ValueError(\n\u001b[0;32m--> 162\u001b[0;31m           \u001b[0;34m'Please use `tf.keras.losses.Reduction.SUM` or '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m           \u001b[0;34m'`tf.keras.losses.Reduction.NONE` for loss reduction when losses are '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m           \u001b[0;34m'used with `tf.distribute.Strategy` outside of the built-in training '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Please use `tf.keras.losses.Reduction.SUM` or `tf.keras.losses.Reduction.NONE` for loss reduction when losses are used with `tf.distribute.Strategy` outside of the built-in training loops. You can implement `tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE` using global batch size like:\n```\nwith strategy.scope():\n    loss_obj = tf.keras.losses.CategoricalCrossentropy(reduction=tf.keras.losses.reduction.NONE)\n....\n    loss = tf.reduce_sum(loss_obj(labels, predictions)) * (1. / global_batch_size)\n```\nPlease see https://www.tensorflow.org/alpha/tutorials/distribute/training_loops for more details."
     ]
    }
   ],
   "source": [
    "\n",
    "with strategy.scope():\n",
    "    model=vgg('vgg16',224,224,5)\n",
    "    model.summary()\n",
    "    \n",
    "    # use keras low level api for training\n",
    "    loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    \n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "    train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "    \n",
    "    test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "    test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "    \n",
    "\n",
    "    def train_step(images,labels):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(images,training=True)\n",
    "            loss = loss_object(labels,predictions)\n",
    "        gradients = tape.gradient(loss,model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients,model.trainable_variables))\n",
    "        \n",
    "        train_loss(loss)\n",
    "        train_accuracy(labels,predictions)\n",
    "        \n",
    "\n",
    "    def test_step(images,labels):\n",
    "        predictions = model(images,training=False)\n",
    "        t_loss = loss_object(labels,predictions)\n",
    "        \n",
    "        test_loss(t_loss)\n",
    "        test_accuracy(labels,predictions)\n",
    "        \n",
    "    best_test_loss = float('inf')\n",
    "    train_step_num = train_num//batch_size\n",
    "    val_step_num = val_num//batch_size\n",
    "    for epoch in range(epochs+1):\n",
    "        train_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "        test_loss.reset_states()\n",
    "        test_accuracy.reset_states()\n",
    "        \n",
    "        t1 = time.perf_counter()\n",
    "        for index,(images,labels) in enumerate(train_dataset):\n",
    "            train_step(images,labels)\n",
    "            if index+1 == train_step_num:\n",
    "                break\n",
    "        print(time.perf_counter-t1)\n",
    "        \n",
    "        for index,(images,labels) in enumerate(val_dataset):\n",
    "            test_step(images,labels)\n",
    "            if index+1 == val_step_num:\n",
    "                break\n",
    "        template = 'Epoch {},loss: {},Accuracy: {},Test Loss: {}'\n",
    "        print(template.format(epoch,\n",
    "                             train_loss.result(),\n",
    "                             train_accuracy.result()*100,\n",
    "                             test_loss.result(),\n",
    "                             test_accuracy.result()*100))\n",
    "        if test_loss.result() < best_test_loss:\n",
    "            model.save_weights('./save_weights/myVGG_{}.ckpt'.format(epoch),save_format='tf')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
