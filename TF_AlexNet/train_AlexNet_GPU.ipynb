{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from AlexNetModel.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from AlexNetModel import AlexNet_v1,AlexNet_v2,AlexNet_v1_2gpu\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_DEVICE_ORDER'] = \"PCI_BUS_ID\"  # os.environ[“CUDA_DEVICE_ORDER”] = “PCI_BUS_ID” # 按照PCI_BUS_ID顺序从0开始排列GPU设备\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'        # 设置当前使用的GPU设备仅为0号设备  设备名称为'/gpu:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "logical_gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu,True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "        exit(-1)\n",
    "logical_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = os.path.abspath(os.path.join(os.getcwd(),\"../../datasets\"))\n",
    "image_path = data_root+\"/flower_data/\"\n",
    "train_dir = image_path+'train'\n",
    "validation_dir = image_path+'val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_height = 224\n",
    "im_width = 224\n",
    "batch_size = 32\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'dandelion', 1: 'daisy', 2: 'roses', 3: 'tulips', 4: 'sunflowers'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class dict\n",
    "data_class = [cla for cla in os.listdir(train_dir) if '.txt' not in cla]\n",
    "class_num = len(data_class)\n",
    "class_dict = dict((value,index) for index,value in enumerate(data_class))\n",
    "inverse_dict = dict((value,key) for key,value in class_dict.items())\n",
    "inverse_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train images list\n",
    "train_image_list = glob.glob(train_dir+\"/*/*.jpg\")\n",
    "random.shuffle(train_image_list)\n",
    "train_num = len(train_image_list)\n",
    "train_label_list = [class_dict[path.split(os.path.sep)[-2]] for path in train_image_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load validation images list\n",
    "val_image_list = glob.glob(validation_dir+\"/*/*.jpg\")\n",
    "random.shuffle(val_image_list)\n",
    "val_num = len(val_image_list)\n",
    "val_label_list = [class_dict[path.split(os.path.sep)[-2]] for path in val_image_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_path(img_path,label):\n",
    "    label = tf.one_hot(label,depth=class_num)\n",
    "    image = tf.io.read_file(img_path)\n",
    "    image = tf.image.decode_jpeg(image)\n",
    "    image = tf.image.convert_image_dtype(image,tf.float32)\n",
    "    image = tf.image.resize(image,[im_height,im_width])\n",
    "    return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()  \n",
    "# batch_size_per_replica = 32\n",
    "# # Global batch size\n",
    "# GLOBAL_BATCH_SIZE = batch_size_per_replica * strategy.num_replicas_in_sync\n",
    "# # Buffer size for data loader\n",
    "# BUFFER_SIZE = batch_size_per_replica * strategy.num_replicas_in_sync * 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_image_list,train_label_list))\n",
    "\"\"\"\n",
    "通过‘’tf.data.Dataset.prefetch‘转换，tf.data’ API提供了一个软件流水线操作机制，可以用来解耦数据产生的时间和数据消耗的时间。\n",
    "特别地，转换使用一个后台线程和一个内部缓冲区，以便在请求输入数据集的元素之前预取它们。\n",
    "预取元素的数量应该等于(或者可能大于)单个训练步骤所消耗的批数。您可以手动调整这个值，\n",
    "或者将其设置为tf.data.experimental.AUTOTUNE,它将提示tf.data runtime在运行时动态地调整值。\n",
    "\"\"\"\n",
    "train_dataset = train_dataset.shuffle(buffer_size=train_num).map(process_path,num_parallel_calls=AUTOTUNE)\\\n",
    "                                .repeat().batch(batch_size).prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_image_list,val_label_list))\n",
    "val_dataset = val_dataset.map(process_path,num_parallel_calls=AUTOTUNE).repeat().batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2 (None, 227, 227, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 55, 55, 96)        34944     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 27, 27, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 27, 27, 256)       614656    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 13, 13, 384)       885120    \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 13, 13, 384)       1327488   \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 13, 13, 256)       884992    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4096)              37752832  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 20485     \n",
      "_________________________________________________________________\n",
      "softmax (Softmax)            (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 58,301,829\n",
      "Trainable params: 58,301,829\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with strategy.scope(): \n",
    "    model = AlexNet_v1_2gpu(im_height=im_height,im_width=im_width,class_num=5)\n",
    "    model.summary() \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "             loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False), #因为已经softmax所以false\n",
    "             metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 103 steps, validate for 11 steps\n",
      "Epoch 1/10\n",
      "INFO:tensorflow:batch_all_reduce: 16 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "103/103 [==============================] - 11s 105ms/step - loss: 0.5762 - accuracy: 0.7743 - val_loss: 0.8086 - val_accuracy: 0.6960\n",
      "Epoch 2/10\n",
      "103/103 [==============================] - 8s 77ms/step - loss: 0.5439 - accuracy: 0.7967 - val_loss: 0.9274 - val_accuracy: 0.6790\n",
      "Epoch 3/10\n",
      "103/103 [==============================] - 8s 77ms/step - loss: 0.4502 - accuracy: 0.8277 - val_loss: 0.9727 - val_accuracy: 0.6676\n",
      "Epoch 4/10\n",
      "103/103 [==============================] - 8s 77ms/step - loss: 0.3838 - accuracy: 0.8474 - val_loss: 0.8582 - val_accuracy: 0.6989\n",
      "Epoch 5/10\n",
      "103/103 [==============================] - 8s 76ms/step - loss: 0.3826 - accuracy: 0.8553 - val_loss: 0.8793 - val_accuracy: 0.7386\n",
      "Epoch 6/10\n",
      "103/103 [==============================] - 8s 76ms/step - loss: 0.2671 - accuracy: 0.8929 - val_loss: 1.3767 - val_accuracy: 0.6818\n",
      "Epoch 7/10\n",
      "103/103 [==============================] - 8s 76ms/step - loss: 0.2501 - accuracy: 0.9053 - val_loss: 1.3453 - val_accuracy: 0.6903\n",
      "Epoch 8/10\n",
      "103/103 [==============================] - 8s 76ms/step - loss: 0.2611 - accuracy: 0.9002 - val_loss: 1.3407 - val_accuracy: 0.6818\n",
      "Epoch 9/10\n",
      "103/103 [==============================] - 8s 78ms/step - loss: 0.2049 - accuracy: 0.9196 - val_loss: 1.2213 - val_accuracy: 0.7131\n",
      "Epoch 10/10\n",
      "103/103 [==============================] - 8s 75ms/step - loss: 0.2036 - accuracy: 0.9296 - val_loss: 1.2214 - val_accuracy: 0.6790\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=train_dataset,\n",
    "                        steps_per_epoch=train_num // batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=val_dataset,\n",
    "                        validation_steps=val_num // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
